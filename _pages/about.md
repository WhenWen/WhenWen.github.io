---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Hello! I am Kaiyue Wen. I am a second-year Phd student at Stanford University, where I am grateful to be advised by  [Tengyu Ma](https://ai.stanford.edu/~tengyuma/) and [Percy Liang](https://cs.stanford.edu/~pliang/). I graduated from Tsinghua University, where I was a member of Yao's pilot class. Here are my [CV](https://whenwen.github.io/files/CVofKaiyueWen.pdf) and [Publications](https://scholar.google.com/citations?hl=en&user=oTmQCFUAAAAJ). During my undergraduate study, I am fortunate to be advised by [Tengyu Ma](https://ai.stanford.edu/~tengyuma/), [Zhiyuan Liu](http://nlp.csai.tsinghua.edu.cn/~lzy/), [Andrej Risteski](https://www.andrew.cmu.edu/user/aristesk/), [Jingzhao Zhang](https://sites.google.com/view/jingzhao/home), [Yuhao Wang](https://yuhaow.github.io/) and [Zhiyuan Li](https://ai.stanford.edu/~zhiyuanli/).

My research interest spreads broadly in deep learning. My long-term goal is to understand the physics behind deep learning and I believe a combination of theoretical analysis and empirical study is essential for this goal.

Recently, Iâ€™ve become fascinated by two fundamental axes of scaling in deep learning.

1. **Demystifying pretraining**: Pretraining has been the driving force behind the evolution of large language models, yet many foundational algorithmic choices remain poorly understood. Key aspects such as optimizers, architectures, and hyperparameter scaling strategies still lack consensus. My goal is to clarify these choices through rigorous benchmarking (e.g., [benchmarking modern optimizers](https://arxiv.org/abs/2509.02046)) and theoretical analysis (e.g., exploring the [representation limitation of RNNs](https://arxiv.org/abs/2402.18510), [architectures beyond $\mathrm{TC}^0$](https://arxiv.org/abs/2505.16381), and [river-valley loss landscape](https://arxiv.org/abs/2410.05192)). Most of my research in this direction is carried out in the open-source project [Marin](https://marin.community/).

2. **New algorithmic paradigms in reasoning**: With the recent progress in reasoning reinforcement learning (RL), particularly innovations like long-chain-of-thought RL, there is growing potential to push the limits of model reasoning. While I am new to this field, my aim is to design end-to-end trainable multi-agent RL systems that build upon and extend the capabilities of current long-CoT RL paradigms.








## Recent News


* **Sep, 2025** New preprint ([Fantastic Pretraining Optimizers and Where to Find Them
](https://arxiv.org/abs/2509.02046)) on arxiv! 
* **May, 2025** [WSD-S](https://arxiv.org/abs/2410.05192) is used in training the best open-source 8B model [Marin 8B](https://marin.readthedocs.io/en/latest/reports/marin-8b-retro/).
* **Jan, 2025** New preprint ([Global Load Balancing Helps Expert Specialization](https://arxiv.org/abs/2501.11873)) on arxiv! 
* **Jan, 2025** 3 papers ([River Valley Landscape](https://arxiv.org/abs/2410.05192), [RNNs are not Transformers (Yet)](https://arxiv.org/abs/2402.18510), [Optimization Analysis on Chain-of-Thought](https://arxiv.org/abs/2410.05459)) accepted at ICLR 2025!
* **Dec, 2024** [Residual Permutation Test](https://arxiv.org/abs/2211.16182) is accepted at AoS!
* **Oct, 2024** New preprints [River Valley Landscape](https://arxiv.org/abs/2410.05192) and [Optimization Analysis on Chain-of-Thought](https://arxiv.org/abs/2410.05459) on arxiv!
* **Sep, 2024** Start my Ph.D. study at Stanford University! I am currently rotating with [Percy Liang](https://cs.stanford.edu/~pliang/).
* **Jul, 2024** Graduated from Tsinghua University with a Bachelor's degree in Computer Science. 
* **May, 2024** Receive and accept the offer from Stanford University!  I am honored to receive the Stanford Graduate Fellowship.
* **Feb, 2024** New preprint [RNNs are not Transformers (Yet)](https://arxiv.org/abs/2402.18510) on arxiv!
* **Oct, 2023** Awarded the National Scholarship (top 0.2%)!
* **Sep, 2023** 2 papers ([Sharpness&amp;Generalization](https://arxiv.org/abs/2307.11007), [(Un)interpretability of Transformers](https://openreview.net/forum?id=kaILSVAspn)) accepted at Neurips 2023! [Sharpness&amp;Generalization](https://arxiv.org/abs/2307.11007) is received as **oral**.
* **Sep, 2023** Receive the silver medal for Yao Award (Top 4 in Yao's pilot class)!
* **Aug, 2023** Return to China for my senior year in Tsinghua.
* **Jul, 2023** Visit Hawaii for ICML 2023! Always great to see old friends.
* **Jun, 2023** [Residual Permutation Test](https://arxiv.org/abs/2211.16182) receive Major Revision from AoS.
* **Jun, 2023** Visiting [Tengyu Ma](https://ai.stanford.edu/~tengyuma/) at Stanford!
* **May, 2023** Visit Rwanda for ICLR 2023!
* **Mar, 2023** Reviewing ICML for the first time!
* **Mar, 2023** New preprint [Solving LPN with Neural Networks](https://arxiv.org/abs/2303.07987) on arxiv!
* **Feb, 2023** Visiting [Andrej Risteski](https://www.andrew.cmu.edu/user/aristesk/) at CMU!
* **Jan, 2023** 2 papers ([Understanding SAM](https://arxiv.org/abs/2211.05729), [Not Benign Overfitting](https://arxiv.org/abs/2206.00501)) accepted at ICLR 2023!
* **Dec, 2022** New preprint [Residual Permutation Test](https://arxiv.org/abs/2211.16182) on arxiv!
* **Dec, 2022** New preprint [Understanding SAM](https://arxiv.org/abs/2211.05729) on arxiv!
* **Oct, 2022** 1 paper ([Skill Neurons](https://arxiv.org/abs/2211.07349)) accepted at EMNLP 2022.
* **Jun, 2022** New preprint [Not Benign Overfitting](https://arxiv.org/abs/2206.00501) on arxiv!

## One More Thing

I keep a firm faith in analytical thinking, hard work, and consistent self-improvement. Any advice or feedback is welcome. You can use this [Anonymous Form](https://www.admonymous.co/kaiyue) or discuss with me in person.
