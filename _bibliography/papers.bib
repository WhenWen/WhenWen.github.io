@inproceedings{su2022transferability,
  title={On Transferability of Prompt Tuning for Natural Language Processing},
  author={Su, Yusheng and Wang, Xiaozhi and Qin, Yujia and Chan, Chi-Min and Lin, Yankai and Wang, Huadong and Wen, Kaiyue and Liu, Zhiyuan and Li, Peng and Li, Juanzi and Hou, Lei and Sun, Maosong and Zhou, Jie},
  booktitle={NAACL},
  year={2022},
  url={https://openreview.net/pdf?id=m80bEyzG4iE},
  abbr={NAACL},
  abstract={This paper discuss how to transfer prompt trained on different tasks and models. I participated in the experiment on predicting transferability using neuron activation.}
}

@inproceedings{wang2022finding,
  title={Finding Skill Neurons in Pre-trained Transformers via Prompt Tuning},
  author={Wang, Xiaozhi and Wen, Kaiyue and Zhang, Zhengyan and Hou, Lei and Liu, Zhiyuan and Li, Juanzi},
  booktitle={EMNLP},
  year={2022},
  url={https://openreview.net/pdf?id=-FFxLqKt33k},
  abbr={EMNLP},
  abstract={This paper discuss our discovery of a set of neurons inside pretrained language model that encode skills, meaning that the activations of these neurons, with delta training, or even without any training, can be used to predict some of the downstream tasks. We further prove that these neurons are crucial for downstream delta tuning.},
  selected={true},
  selected_index={4}
}

@misc{wen2023benignoverfittingclassificationprovably,
      title={Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models}, 
      author={Kaiyue Wen and Jiaye Teng and Jingzhao Zhang},
      year={2023},
      eprint={2206.00501},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2206.00501},
      arxiv={2206.00501},
      abbr={ICLR}
}

@misc{wen2023doessharpnessawareminimizationminimize,
      title={How Does Sharpness-Aware Minimization Minimize Sharpness?}, 
      author={Kaiyue Wen and Tengyu Ma and Zhiyuan Li},
      year={2023},
      eprint={2211.05729},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.05729},
      arxiv={2211.05729},
      selected={true},
      selected_index={3},
      abbr={ICLR}
}

@misc{wen2025residualpermutationtestregression,
      title={Residual permutation test for regression coefficient testing}, 
      author={Kaiyue Wen and Tengyao Wang and Yuhao Wang},
      year={2025},
      eprint={2211.16182},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      doi={https://doi.org/10.1214/24-AOS2479},
      url={https://arxiv.org/abs/2211.16182},
      arxiv={2211.16182},
      abbr={AoS}
}

@misc{jiang2023practicallysolvinglpnhigh,
      title={Practically Solving LPN in High Noise Regimes Faster Using Neural Networks}, 
      author={Haozhe Jiang and Kaiyue Wen and Yilei Chen},
      year={2023},
      eprint={2303.07987},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.07987},
      arxiv={2303.07987},
      abbr={arXiv}
}

@misc{wen2025fantasticpretrainingoptimizers,
      title={Fantastic Pretraining Optimizers and Where to Find Them}, 
      author={Kaiyue Wen and David Hall and Tengyu Ma and Percy Liang},
      year={2025},
      eprint={2509.02046},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2509.02046},
      arxiv={2509.02046},
      arxiv={2509.02046},
      selected={true},
      selected_index={2},
      abbr={arXiv}
}

@misc{qiu2025demonsdetailimplementingload,
      title={Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models}, 
      author={Zihan Qiu and Zeyu Huang and Bo Zheng and Kaiyue Wen and Zekun Wang and Rui Men and Ivan Titov and Dayiheng Liu and Jingren Zhou and Junyang Lin},
      year={2025},
      eprint={2501.11873},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.11873},
      arxiv={2501.11873},
      abbr={ACL}
}

@misc{wen2024understandingwarmupstabledecaylearningrates,
      title={Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective}, 
      author={Kaiyue Wen and Zhiyuan Li and Jason Wang and David Hall and Percy Liang and Tengyu Ma},
      year={2024},
      eprint={2410.05192},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05192},
      arxiv={2410.05192},
      selected={true},
      selected_index={1},
      abbr={ICLR}
}

@misc{wen2024rnnstransformersyetkey,
      title={RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval}, 
      author={Kaiyue Wen and Xingyu Dang and Kaifeng Lyu},
      year={2024},
      eprint={2402.18510},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18510},
      arxiv={2402.18510},
      abbr={ICLR}
}

@misc{wen2025sparsedependencesparseattention,
      title={From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency}, 
      author={Kaiyue Wen and Huaqing Zhang and Hongzhou Lin and Jingzhao Zhang},
      year={2025},
      eprint={2410.05459},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.05459},
      arxiv={2410.05459},
      abbr={ICLR}
}

@misc{wen2023sharpnessminimizationalgorithmsminimize,
      title={Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization}, 
      author={Kaiyue Wen and Zhiyuan Li and Tengyu Ma},
      year={2023},
      eprint={2307.11007},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.11007},
      arxiv={2307.11007},
      abbr={NeurIPS}
}

@misc{wen2023uninterpretabilitytransformerscase,
      title={(Un)interpretability of Transformers: a case study with Dyck grammars}, 
      author={Kaiyue Wen and Yuchen Li and Bingbin Liu and Andrej Risteski},
      year={2023},
      eprint={2306.17844},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.17844},
      arxiv={2306.17844},
      abbr={NeurIPS}
}

@misc{qiu2025gatedattentionlargelanguage,
      title={Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free}, 
      author={Zihan Qiu and Zekun Wang and Bo Zheng and Zeyu Huang and Kaiyue Wen and Songlin Yang and Rui Men and Le Yu and Fei Huang and Suozhi Huang and Dayiheng Liu and Jingren Zhou and Junyang Lin},
      year={2025},
      eprint={2505.06708},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.06708},
      arxiv={2505.06708},
      abbr={NeurIPS}
}

@misc{wen2025weightensemblingimprovesreasoning,
      title={Weight Ensembling Improves Reasoning in Language Models}, 
      author={Xingyu Dang, Christina Baek, Kaiyue Wen, Zico Kolter, Aditi Raghunathan},
      year={2025},
      eprint={2504.10478},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2504.10478},
      arxiv={2504.10478},
      abbr={COLM}
}

@misc{yang2025pathattentionpositionencoding,
      title={PaTH Attention: Position Encoding via Accumulating Householder Transformations}, 
      author={Songlin Yang and Yikang Shen and Kaiyue Wen and Shawn Tan and Mayank Mishra and Liliang Ren and Rameswar Panda and Yoon Kim},
      year={2025},
      eprint={2505.16381},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.16381},
      arxiv={2505.16381},
      abbr={NeurIPS}
}

@misc{wen2025taskgeneralizationautoregressive,
      title={Task Generalization With AutoRegressive Compositional Structure: Can Learning From $D$ Tasks Generalize to $D^{T}$ Tasks?}, 
      author={Kaiyue Wen and Jikai Jin and Yuhao Wang and Zhiyuan Li},
      year={2025},
      eprint={2502.08991},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.08991},
      arxiv={2502.08991},
      abbr={ICML}
}
