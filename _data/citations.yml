metadata:
  last_updated: '2026-01-28'
papers:
  oTmQCFUAAAAJ:0EnyYjriUFMC:
    citations: 106
    title: Finding Skill Neurons in Pre-trained Transformer-based Language Models
    year: '2022'
  oTmQCFUAAAAJ:4DMP91E08xMC:
    citations: 6
    title: 'Task Generalization With AutoRegressive Compositional Structure: Can Learning From  Tasks Generalize to  Tasks?'
    year: '2025'
  oTmQCFUAAAAJ:4TOpqqG69KYC:
    citations: 21
    title: 'From sparse dependence to sparse attention: unveiling how chain-of-thought enhances transformer sample efficiency'
    year: '2024'
  oTmQCFUAAAAJ:5nxA0vEk-isC:
    citations: 0
    title: Practically Solving LPN in High Noise Regimes Faster Using Neural Networks
    year: '2023'
  oTmQCFUAAAAJ:7PzlFSSx8tAC:
    citations: 23
    title: Fantastic pretraining optimizers and where to find them
    year: '2025'
  oTmQCFUAAAAJ:8k81kl-MbHgC:
    citations: 12
    title: 'Benign overfitting in classification: Provably counter label noise with larger models'
    year: '2022'
  oTmQCFUAAAAJ:9ZlFYXVOiuMC:
    citations: 14
    title: Residual permutation test for regression coefficient testing
    year: '2025'
  oTmQCFUAAAAJ:IWHjjKOFINEC:
    citations: 5
    title: 'Understanding warmup-stable-decay learning rates: A river valley loss landscape perspective, 2024'
    year: Unknown Year
  oTmQCFUAAAAJ:IjCSPb-OGe4C:
    citations: 186
    title: On transferability of prompt tuning for natural language processing
    year: '2022'
  oTmQCFUAAAAJ:KlAtU1dfN6UC:
    citations: 36
    title: 'Transformers are uninterpretable with myopic methods: a case study with bounded dyck grammars'
    year: '2023'
  oTmQCFUAAAAJ:QIV2ME_5wuYC:
    citations: 38
    title: 'Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free'
    year: '2025'
  oTmQCFUAAAAJ:ULOm3_A8WrAC:
    citations: 47
    title: 'Rnns are not transformers (yet): The key bottleneck on in-context retrieval'
    year: '2024'
  oTmQCFUAAAAJ:UebtZRa9Y70C:
    citations: 125
    title: How Sharpness-Aware Minimization Minimizes Sharpness?
    year: Unknown Year
  oTmQCFUAAAAJ:Wp0gIr-vW9MC:
    citations: 13
    title: Weight ensembling improves reasoning in language models
    year: '2025'
  oTmQCFUAAAAJ:ZeXyd9-uunAC:
    citations: 6
    title: 'Questa: Expanding reasoning capacity in llms via question augmentation'
    year: '2025'
  oTmQCFUAAAAJ:_kc_bZDykSQC:
    citations: 31
    title: 'Understanding warmup-stable-decay learning rates: A river valley loss landscape perspective'
    year: '2024'
  oTmQCFUAAAAJ:aqlVkmm33-oC:
    citations: 23
    title: 'Demons in the detail: On implementing load balancing loss for training specialized mixture-of-expert models'
    year: '2025'
  oTmQCFUAAAAJ:dhFuZR0502QC:
    citations: 10
    title: 'PaTH Attention: Position Encoding via Accumulating Householder Transformations'
    year: '2025'
  oTmQCFUAAAAJ:kNdYIx-mwKoC:
    citations: 56
    title: Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization
    year: '2023'
  oTmQCFUAAAAJ:mVmsd5A6BfQC:
    citations: 30
    title: Overtrained language models are harder to fine-tune
    year: '2025'
