metadata:
  last_updated: '2025-12-08'
papers:
  oTmQCFUAAAAJ:0EnyYjriUFMC:
    citations: 103
    title: Finding Skill Neurons in Pre-trained Transformer-based Language Models
    year: '2022'
  oTmQCFUAAAAJ:4DMP91E08xMC:
    citations: 5
    title: 'Task Generalization With AutoRegressive Compositional Structure: Can Learning From  Tasks Generalize to  Tasks?'
    year: '2025'
  oTmQCFUAAAAJ:4TOpqqG69KYC:
    citations: 18
    title: 'From sparse dependence to sparse attention: unveiling how chain-of-thought enhances transformer sample efficiency'
    year: '2024'
  oTmQCFUAAAAJ:5nxA0vEk-isC:
    citations: 0
    title: Practically Solving LPN in High Noise Regimes Faster Using Neural Networks
    year: '2023'
  oTmQCFUAAAAJ:7PzlFSSx8tAC:
    citations: 17
    title: Fantastic pretraining optimizers and where to find them
    year: '2025'
  oTmQCFUAAAAJ:8k81kl-MbHgC:
    citations: 12
    title: 'Benign overfitting in classification: Provably counter label noise with larger models'
    year: '2022'
  oTmQCFUAAAAJ:9ZlFYXVOiuMC:
    citations: 12
    title: Residual permutation test for regression coefficient testing
    year: '2025'
  oTmQCFUAAAAJ:IjCSPb-OGe4C:
    citations: 183
    title: On transferability of prompt tuning for natural language processing
    year: '2022'
  oTmQCFUAAAAJ:KlAtU1dfN6UC:
    citations: 34
    title: 'Transformers are uninterpretable with myopic methods: a case study with bounded dyck grammars'
    year: '2023'
  oTmQCFUAAAAJ:QIV2ME_5wuYC:
    citations: 16
    title: 'Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free'
    year: '2025'
  oTmQCFUAAAAJ:ULOm3_A8WrAC:
    citations: 46
    title: 'Rnns are not transformers (yet): The key bottleneck on in-context retrieval'
    year: '2024'
  oTmQCFUAAAAJ:UebtZRa9Y70C:
    citations: 122
    title: How Sharpness-Aware Minimization Minimizes Sharpness?
    year: Unknown Year
  oTmQCFUAAAAJ:Wp0gIr-vW9MC:
    citations: 12
    title: Weight ensembling improves reasoning in language models
    year: '2025'
  oTmQCFUAAAAJ:_kc_bZDykSQC:
    citations: 26
    title: 'Understanding warmup-stable-decay learning rates: A river valley loss landscape perspective'
    year: '2024'
  oTmQCFUAAAAJ:aqlVkmm33-oC:
    citations: 12
    title: 'Demons in the detail: On implementing load balancing loss for training specialized mixture-of-expert models'
    year: '2025'
  oTmQCFUAAAAJ:dhFuZR0502QC:
    citations: 7
    title: 'PaTH Attention: Position Encoding via Accumulating Householder Transformations'
    year: '2025'
  oTmQCFUAAAAJ:kNdYIx-mwKoC:
    citations: 53
    title: Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization
    year: '2023'
  oTmQCFUAAAAJ:mVmsd5A6BfQC:
    citations: 25
    title: Overtrained language models are harder to fine-tune
    year: '2025'
