Key References on Weight Decay and Effective Learning Rate

Weight Decay in Scale-Invariant Networks

- **Van Laarhoven (2017)** – Demonstrated that in networks with BatchNorm, **L2 weight decay has no actual regularization effect on the loss**, but instead *influences the scale of the weights and thus the effective learning rate*. In other words, scaling the weights by a constant leaves the BatchNorm outputs (and loss) unchanged[[1\]](https://arxiv.org/abs/1706.05350%23:~:text=of%20deep%20neural%20networks,ways%20to%20mitigate%20this%20issue). This was an early indication that weight decay in normalized networks doesn’t constrain capacity but alters optimization dynamics.
- **Zhang et al. (2019)** – Showed empirically that **weight decay has no effect on layers with BatchNorm** in theory (since one can rescale weights without changing predictions), yet it still improves accuracy by acting as an **increase to the effective learning rate**[[2\]](https://iclr-blogposts.github.io/2023/blog/2023/adamw/%23:~:text=,a%20rescaling%20of%20the%20weights)[[3\]](https://iclr-blogposts.github.io/2023/blog/2023/adamw/%23:~:text=,an%20effectively%20larger%20learning%20rate). They identified *“increasing the effective learning rate”* as one of three primary mechanisms by which weight decay influences training[[4\]](https://openreview.net/forum?id=B1lz-3Rct7%23:~:text=variety%20of%20network%20architectures,the%20regularization%20of%20neural%20networks). This refutes the classical view of weight decay as capacity control in modern normalized networks.
- **Hoffer et al. (2018)** – Provided a “*Norm matters*” perspective: **batch normalization and weight decay serve to decouple a weight vector’s norm from the objective**, effectively letting the optimizer focus on the weight direction[[5\]](https://www.researchgate.net/publication/323571095_Norm_matters_efficient_and_accurate_normalization_schemes_in_deep_networks%23:~:text=present%20a%20novel%20view%20on,precision%20implementations). They note that when normalization is present, weight decay keeps the weight norm in a small range, thereby stabilizing training – a hint at weight decay’s role as an optimizer tweak rather than a generalization penalty.

Weight Decay as Effective Learning Rate (Intrinsic LR)

- **Loshchilov & Hutter (2019)** – Introduced **AdamW** optimizer, decoupling weight decay from the gradient update[[6\]](https://iclr-blogposts.github.io/2023/blog/2023/adamw/%23:~:text=How%20would%20Adam%20handle%20regularization?,was%20the%20development%20of%20AdamW)[[7\]](https://arxiv.org/html/2506.02285v1%23:~:text=Loshchilov%20and%20Hutter,%20,1). This highlights that weight decay can be applied as a simple weight shrinkage each step, reinforcing the idea that it’s an optimizer hyperparameter controlling the update magnitude rather than a direct regularizer of loss.
- **Li, Lyu & Arora (2020)** – Theoretically **proved that weight decay in scale-invariant settings introduces an “intrinsic learning rate”** equal to the product $\eta \lambda$ (learning rate times weight decay). They showed that in networks with normalization, training reaches an equilibrium that depends *only* on this intrinsic LR, independent of initial scale or prior history[[8\]](https://proceedings.neurips.cc/paper/2020/file/a7453a5f026fb6831d68bdc9cb0edcae-Supplemental.pdf%23:~:text=distribution%20in%20the%20function%20space,main%20contributions%20are%20the%20following). In their **“Fast Equilibrium”** conjecture, they observed that at equilibrium the weight norm is set by $\eta\lambda$, which directly controls the effective step size of parameter direction changes. This formalizes the notion that **tuning $\eta \lambda$ is what actually matters**, aligning with later empirical findings.
- **van Laarhoven (2017)** – Also noted that **effective step size is governed by weight norm**: weight decay keeps the weight norm from growing, thus preventing the learning rate from effectively becoming too large[[1\]](https://arxiv.org/abs/1706.05350%23:~:text=of%20deep%20neural%20networks,ways%20to%20mitigate%20this%20issue). His analysis in normalized networks anticipated that adjusting weight decay changes the effective learning rate seen by the weights.

Weight Norm Dynamics and Rotational Equilibrium

- **Kosson et al. (2024)** – Proposed that under weight decay, **weights enter a “rotational equilibrium” on a fixed-radius sphere**. The weight norm quickly converges to a stable value set by hyperparameters, and thereafter updates mainly change the weight *direction* (a rotation)[[9\]](https://arxiv.org/html/2510.15262v1%23:~:text=Recent%20works%20(Kosson%20et%C2%A0al,update%20behaves%20like%20a%20rotation). They observed that this equilibrium weight norm is determined by the learning rate and weight decay (and not by initialization), echoing the intrinsic LR idea. In fact, scaling learning rate and weight decay proportionally leaves the model’s layer-wise weight norms and even singular value spectrum almost unchanged[[10\]](https://arxiv.org/html/2510.15262v1%23:~:text=The%20results,%20shown%20in%20Figure,shape%20remains%20stable%20across%20runs). This explains why keeping $\eta \lambda$ constant yields similar training trajectories.
- **Defazio (2025)** – Showed that for layers with normalization (LayerNorm/BatchNorm), **weight decay controls the ratio of gradient norm to weight norm**, enforcing a balance that keeps weights on a fixed sphere[[11\]](https://arxiv.org/html/2506.02285v1%23:~:text=We%20utilize%20an%20existing%20theoretical,the%20weight%20norm%20during%20training). As the learning rate schedule changes, this ratio changes, which **explains phenomena like the gradient norm spike at the end of training**. Defazio attributes the end-of-training gradient explosion in large LLMs to the interaction of weight decay with a decaying learning rate[[12\]](https://arxiv.org/html/2506.02285v1%23:~:text=During%20long,lower%20loss%20values%20throughout%20training)[[11\]](https://arxiv.org/html/2506.02285v1%23:~:text=We%20utilize%20an%20existing%20theoretical,the%20weight%20norm%20during%20training). He further provides a correction to weight decay to maintain constant effective step size and eliminate that spike[[12\]](https://arxiv.org/html/2506.02285v1%23:~:text=During%20long,lower%20loss%20values%20throughout%20training)[[13\]](https://arxiv.org/html/2506.02285v1%23:~:text=We%20propose%20a%20theory,improve%20over%20the%20base%20optimizers).
- **Wen et al. (2025)** – (Referencing *“Rotational Equilibrium: How Weight Decay Balances Learning…”*) arrived at the striking result that **AdamW-trained models have a predictable asymptotic weight RMS** that depends only on optimizer hyperparameters[[14\]](https://kexue.fm/archives/11307%23:~:text=%E5%9C%A8%E3%80%8A%E4%B8%BA%E4%BB%80%E4%B9%88Adam%E7%9A%84Update%20RMS%E6%98%AF0,Across%20Neural%20Networks%E3%80%8B%E4%B8%AD%E3%80%82%E9%98%85%E8%AF%BB%E5%90%8E%EF%BC%8C%E7%AC%94%E8%80%85%E5%8F%91%E7%8E%B0%E5%85%B6%E4%B8%AD%E4%B8%8D%E4%BB%85%E5%8C%85%E5%90%AB%E4%BA%86Update%20RMS%E7%9A%84%E4%BC%B0%E8%AE%A1%EF%BC%8C%E8%BF%98%E5%8C%85%E5%90%AB%E4%BA%86Weight%20RMS%E7%9A%84%E4%BC%B0%E8%AE%A1%E3%80%82). In other words, the weight norm in each layer can be estimated from $(\eta,\lambda,\beta)$ – confirming that weight decay indeed pins the weight norm to a formula. This aligns with the idea that weight decay directly sets the equilibrium radius of the weights, which in turn fixes the effective learning rate on the weight sphere.

Empirical Phenomena in Training Dynamics

- **Li, Lyu & Arora (2020)** – Observed that during each phase of a step-decay learning rate schedule, a BatchNorm-network with weight decay **quickly equilibrates to a new weight norm** determined by the current $\eta \lambda$, losing memory of the previous phase[[8\]](https://proceedings.neurips.cc/paper/2020/file/a7453a5f026fb6831d68bdc9cb0edcae-Supplemental.pdf%23:~:text=distribution%20in%20the%20function%20space,main%20contributions%20are%20the%20following). This explains why, for instance, when the learning rate is warmed up or decayed, the weight norms in each layer track those changes (the weight norms grow during LR warmup and shrink when LR decays). The equilibrium is solely a function of $\eta$ and $\lambda$ in that phase, consistent with training curves where weight norms mirror the learning rate schedule.
- **Defazio (2025)** – Provided an explanation for why **gradient norms often increase in late training** even as loss keeps decreasing: when learning rate is lowered (e.g. during cosine or linear decay), the equilibrium weight norm drops, and due to scale-invariance, a smaller weight norm implies a larger ratio of gradient norm to weight norm[[15\]](https://arxiv.org/html/2506.02285v1%23:~:text=show%20that%20weight%20decay%20controls,the%20weight%20norm%20during%20training). Empirically this manifests as rising gradient magnitudes toward the end of training[[12\]](https://arxiv.org/html/2506.02285v1%23:~:text=During%20long,lower%20loss%20values%20throughout%20training), an effect previously noted by Xie et al. (2023) and others. Defazio’s analysis solidified that this is a predictable outcome of weight decay + LR schedule rather than a mysterious “gradient explosion.”
- **Zhang et al. (2019)** – Noted empirically that if one **holds $\eta \cdot \lambda$ constant, increasing $\eta$ while decreasing $\lambda$ (or vice versa) leads to similar outcomes**, both in final loss and in weight norms. This is because only the product matters for the effective step size[[3\]](https://iclr-blogposts.github.io/2023/blog/2023/adamw/%23:~:text=,an%20effectively%20larger%20learning%20rate). In practice, when $\eta\lambda$ is fixed, models converge to essentially the same performance while the weight norm in each layer scales approximately $\propto \sqrt{\eta/\lambda}$ (larger $\eta$, smaller $\lambda$ gives larger equilibrium norms, and vice versa). This phenomenon underlies why different $(\eta,\lambda)$ pairs can lead to the “same” training dynamics if their product is equal – a point also backed by theory[[8\]](https://proceedings.neurips.cc/paper/2020/file/a7453a5f026fb6831d68bdc9cb0edcae-Supplemental.pdf%23:~:text=distribution%20in%20the%20function%20space,main%20contributions%20are%20the%20following).
- **Wen et al. (2024)** – Introduced the **“River Valley” loss landscape perspective** to explain why a high weight decay (hence higher effective LR) run can start off at higher loss but end up converging lower. In a non-convex valley-shaped loss, a higher effective step size causes the iterate to oscillate or “bounce” against the valley walls (higher loss initially) but also speeds progress down the valley floor[[16\]](https://openreview.net/pdf?id=m51BgoqvbP%23:~:text=phenomenon,%20we%20conjecture%20that%20pretraining,Under%20this%20assumption)[[17\]](https://openreview.net/pdf?id=m51BgoqvbP%23:~:text=the%20sustained%20high%20learning%20rate,only%20one%20main%20branch,%20where). When the learning rate (or effective step size) is later reduced, the oscillations dampen and the model quickly descends to a much lower loss – lower than a model that moved cautiously all along. This theory explains the empirical observation that **with weight decay, training loss might plateau higher in early epochs yet ultimately reach a better minimum** (compared to no-decay training)[[16\]](https://openreview.net/pdf?id=m51BgoqvbP%23:~:text=phenomenon,%20we%20conjecture%20that%20pretraining,Under%20this%20assumption)[[17\]](https://openreview.net/pdf?id=m51BgoqvbP%23:~:text=the%20sustained%20high%20learning%20rate,only%20one%20main%20branch,%20where). A related analysis by D’Angelo et al. (2024) also found that weight decay improves training loss by maintaining a larger effective learning rate without diverging, especially important in large-batch or one-pass training[[18\]](https://arxiv.org/html/2506.02285v1%23:~:text=Kosson%20et%C2%A0al,blowup%20when%20training%20in%20bfloat16).
- **D’Angelo et al. (2024)** – In *“Why Do We Need Weight Decay in Modern Deep Learning?”* argued that **weight decay is not useful as an explicit regularizer in deep nets, but it** ***improves optimization dynamics\***. For vision tasks, they describe weight decay as aiding a loss stabilization mechanism (complementing SGD’s implicit regularization), and for large language models, weight decay provides a bias-variance trade-off that yields lower training loss and prevents instabilities (like overflow with float16)[[19\]](https://www.researchgate.net/publication/323571095_Norm_matters_efficient_and_accurate_normalization_schemes_in_deep_networks%23:~:text=regularization%20effect%20studied%20in%20classical,training%20dynamics%20in%20a%20desirable). This reinforces that the primary benefit of weight decay is **faster or more stable convergence**, rather than capacity control or classic regularization[[19\]](https://www.researchgate.net/publication/323571095_Norm_matters_efficient_and_accurate_normalization_schemes_in_deep_networks%23:~:text=regularization%20effect%20studied%20in%20classical,training%20dynamics%20in%20a%20desirable).

Hyperparameter Transfer and Width/Depth Scaling

- **Yang et al. (2021)** – (Maximal Update Parametrization, μP) theorized rules for scaling learning rate and initialization with network width to preserve feature learning dynamics[[20\]](https://arxiv.org/html/2510.15262v1%23:~:text=Maximal,to%20the%20model%20width)[[21\]](https://arxiv.org/html/2510.15262v1%23:~:text=magnitudes%20are%20dominated%20by%20the,Their%20prescription%20yields). However, subsequent empirical work found that μP’s prescriptions alone were not sufficient. In particular, the handling of weight decay became a crucial factor.
- **Kosson et al. (2025)** – Performed a large-scale study and found that **weight decay tuning is more critical than μP for transferring learning rates across model sizes**. They showed that μP’s assumptions (about alignment of gradients and features) hold only briefly at the start of training; after that, **independent weight decay scaling (decay rate not tied to LR) is what stabilizes representation updates across widths**, enabling successful LR transfer[[22\]](https://arxiv.org/html/2510.19093v1%23:~:text=alignment%20of%20a%20layer%E2%80%99s%20inputs,practice%20such%20as%20why%20P)[[23\]](https://arxiv.org/html/2510.19093v1%23:~:text=Image:%20Refer%20to%20caption%20,between,%20see%20Appx.%C2%A0B). In fact, they demonstrated that using the PyTorch-style “independent” weight decay (where you keep $\lambda$ fixed instead of scaling it with LR) overrides μP’s intended scaling and yields much better transfer performance (Figure 1 in their paper)[[23\]](https://arxiv.org/html/2510.19093v1%23:~:text=Image:%20Refer%20to%20caption%20,between,%20see%20Appx.%C2%A0B). This explains practical lore: to transfer hyperparameters from a small model to a large model, one must **properly scale weight decay with width/depth** – otherwise the larger model’s weight norms drift, breaking the transfer. Kosson et al. fundamentally challenge the belief that μP alone solves width scaling, showing instead that **weight decay’s role (keeping weight norms in check across scales) is the dominant factor**.
- **Wortsman et al. (2023)**, **Wang & Aitchison (2025)**, *etc.* – Also reported that **learning rate schedules only transfer well when using decoupled (constant) weight decay** rather than scaling it with LR[[24\]](https://arxiv.org/html/2510.19093v1%23:~:text=Despite%20this%20success,%20several%20works%C2%A0,scaled%20in%20the%20formulation%20above). Without weight decay, or with naïve scaling, larger networks ended up with different effective step sizes, hampering transfer. These observations are consistent with the notion that controlling the norm growth via weight decay is essential for maintaining similar training dynamics in scaled-up models[[25\]](https://arxiv.org/html/2510.19093v1%23:~:text=In%20Appendix%C2%A0B%20we%20explore%20the,reason%20learning%20rates%20somewhat%20transfer) (in fact, in the absence of weight decay, scale-invariant weights will grow until their norm is proportional to the learning rate, which inadvertently makes updates size roughly constant relative to weights[[26\]](https://arxiv.org/html/2510.19093v1%23:~:text=In%20Appendix%C2%A0B%20we%20explore%20the,reason%20learning%20rates%20somewhat%20transfer), but also can lead to degraded performance). Overall, proper weight decay usage has emerged as a cornerstone for hyperparameter transfer in deep learning.

Normalization, Weight Norm, and Model Expressiveness

- **Salimans & Kingma (2016)** – Introduced **Weight Normalization**, a reparameterization that **decouples the length (norm) of weight vectors from their direction**[[27\]](http://papers.neurips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf%23:~:text=,those%20weight%20vectors%20from%20their). Each weight vector $w$ is expressed as $w = \frac{g}{|v|} v$ with a learnable scalar $g$ (norm) and a direction $v$. This decoupling accelerates training by allowing the network to adjust weight magnitudes independently of directions. Crucially, it shows that one can fix or control weight norms without reducing a network’s representational power – the scaling factor $g$ absorbs what would have been the norm.
- **Ioffe & Szegedy (2015)** – (BatchNorm) and later **LayerNorm / RMSNorm** techniques similarly make networks *homogeneous* or scale-invariant in certain layers. **RMSNorm** in Transformers, for example, normalizes the pre-activation vector and then scales it by a learned gain $\gamma$. This means if we scale a weight matrix $W$ by any factor $c>0$ and scale the corresponding RMSNorm gain by $1/c$, the network’s output is unchanged[[14\]](https://kexue.fm/archives/11307%23:~:text=%E5%9C%A8%E3%80%8A%E4%B8%BA%E4%BB%80%E4%B9%88Adam%E7%9A%84Update%20RMS%E6%98%AF0,Across%20Neural%20Networks%E3%80%8B%E4%B8%AD%E3%80%82%E9%98%85%E8%AF%BB%E5%90%8E%EF%BC%8C%E7%AC%94%E8%80%85%E5%8F%91%E7%8E%B0%E5%85%B6%E4%B8%AD%E4%B8%8D%E4%BB%85%E5%8C%85%E5%90%AB%E4%BA%86Update%20RMS%E7%9A%84%E4%BC%B0%E8%AE%A1%EF%BC%8C%E8%BF%98%E5%8C%85%E5%90%AB%E4%BA%86Weight%20RMS%E7%9A%84%E4%BC%B0%E8%AE%A1%E3%80%82). Therefore, as the blog points out, **constraining the Frobenius norm of $W$ (as Hyperball does) does not hurt expressiveness** – any needed overall scaling can be reintroduced by the normalization’s gain parameter. In formula: $f(h; cW,\, \gamma/c) = f(h; W,\, \gamma)$[[14\]](https://kexue.fm/archives/11307%23:~:text=%E5%9C%A8%E3%80%8A%E4%B8%BA%E4%BB%80%E4%B9%88Adam%E7%9A%84Update%20RMS%E6%98%AF0,Across%20Neural%20Networks%E3%80%8B%E4%B8%AD%E3%80%82%E9%98%85%E8%AF%BB%E5%90%8E%EF%BC%8C%E7%AC%94%E8%80%85%E5%8F%91%E7%8E%B0%E5%85%B6%E4%B8%AD%E4%B8%8D%E4%BB%85%E5%8C%85%E5%90%AB%E4%BA%86Update%20RMS%E7%9A%84%E4%BC%B0%E8%AE%A1%EF%BC%8C%E8%BF%98%E5%8C%85%E5%90%AB%E4%BA%86Weight%20RMS%E7%9A%84%E4%BC%B0%E8%AE%A1%E3%80%82). This invariance has been noted in prior work that analyzed BN+WD interactions[[28\]](https://arxiv.org/abs/1706.05350%23:~:text=We%20show%20that%20L2%20regularization,on%20the%20scale%20of%20weights)[[29\]](https://openreview.net/forum?id=B1lz-3Rct7%23:~:text=As%20discussed%20in%20van%20Laarhoven,it%20is%20quite%20similar), and it underpins why one can successfully train with a fixed weight norm.
- **Hoffer et al. (2018)** – Emphasized that **weight decay’s primary role with normalization is to fix the weight norm to a “good” range**, which indirectly sets an effective learning rate for that layer[[5\]](https://www.researchgate.net/publication/323571095_Norm_matters_efficient_and_accurate_normalization_schemes_in_deep_networks%23:~:text=present%20a%20novel%20view%20on,precision%20implementations)[[30\]](https://dl.acm.org/doi/pdf/10.5555/3326943.3327143%23:~:text=,to%20a%20more%20stable). By keeping norms bounded, weight decay makes the optimization landscape smoother in practice and prevents the pathological case (noted by Soudry et al. 2018; Ji & Telgarsky 2019) where in a separable task without weight decay, the weight norms grow to infinity while minimizing the loss[[31\]](https://www.researchgate.net/publication/323571095_Norm_matters_efficient_and_accurate_normalization_schemes_in_deep_networks%23:~:text=,). In summary, controlling the norm (either via weight decay or explicit normalization constraints) is beneficial for training dynamics, and modern architectures include mechanisms (like trainable scale parameters in normalization layers) to ensure that **fixing a weight’s norm doesn’t limit the model’s capacity**.

Frobenius vs. Spectral Norm Control

- **Wen et al. (2023)** – (*“Muon is Scalable for LLM Training”*) observed that transformer weight matrices tend to **maintain full-rank, with a relatively flat singular value spectrum**[[32\]](https://arxiv.org/html/2510.15262v1%23:~:text=Image:%20Refer%20to%20caption)[[33\]](https://arxiv.org/html/2510.15262v1%23:~:text=Although%20the%20alignment%20factor%20cannot,rule%20for%20the%20weight%20decay). In their experiments, using weight decay to constrain the Frobenius norm of weight matrices effectively also kept the largest singular value in check, because none of the singular values dominate overwhelmingly. In practice, they found that scaling the learning rate and weight decay scales **all** singular values nearly uniformly, without changing their distribution shape[[10\]](https://arxiv.org/html/2510.15262v1%23:~:text=The%20results,%20shown%20in%20Figure,shape%20remains%20stable%20across%20runs). Thus, **controlling the Frobenius norm is an effective proxy for controlling the spectral norm** in such models. This justifies Hyperball’s approach of normalizing the entire weight (Frobenius norm) rather than each weight’s spectral norm. As long as the weight retains full rank (which empirical SVDs show is the case in large networks[[34\]](https://arxiv.org/html/2510.15262v1%23:~:text=Although%20the%20alignment%20factor%20cannot,rule%20for%20the%20weight%20decay)), keeping $|W|_F$ constant will also prevent any single singular value from exploding. (By contrast, if the spectrum were highly skewed, one might need spectral norm regularization – but transformers don’t typically exhibit pathological singular value spectra[[35\]](https://arxiv.org/html/2510.15262v1%23:~:text=matrix,shows%20the%20corresponding%20singular%20value).) In summary, **empirical evidence shows Frobenius norm regulation already yields spectral norm stability** for deep networks, simplifying the optimizer design.

![pastedGraphic.png](blob:file:///16151cd6-307d-424f-934c-3276e2b39d4f)

[[1\]](https://arxiv.org/abs/1706.05350%23:~:text=of%20deep%20neural%20networks,ways%20to%20mitigate%20this%20issue) [[28\]](https://arxiv.org/abs/1706.05350%23:~:text=We%20show%20that%20L2%20regularization,on%20the%20scale%20of%20weights) [1706.05350] L2 Regularization versus Batch and Weight Normalization

https://arxiv.org/abs/1706.05350

[[2\]](https://iclr-blogposts.github.io/2023/blog/2023/adamw/%23:~:text=,a%20rescaling%20of%20the%20weights) [[3\]](https://iclr-blogposts.github.io/2023/blog/2023/adamw/%23:~:text=,an%20effectively%20larger%20learning%20rate) [[6\]](https://iclr-blogposts.github.io/2023/blog/2023/adamw/%23:~:text=How%20would%20Adam%20handle%20regularization?,was%20the%20development%20of%20AdamW) Decay No More | ICLR Blogposts 2023

https://iclr-blogposts.github.io/2023/blog/2023/adamw/

[[4\]](https://openreview.net/forum?id=B1lz-3Rct7%23:~:text=variety%20of%20network%20architectures,the%20regularization%20of%20neural%20networks) [[29\]](https://openreview.net/forum?id=B1lz-3Rct7%23:~:text=As%20discussed%20in%20van%20Laarhoven,it%20is%20quite%20similar) Three Mechanisms of Weight Decay Regularization | OpenReview

https://openreview.net/forum?id=B1lz-3Rct7

[[5\]](https://www.researchgate.net/publication/323571095_Norm_matters_efficient_and_accurate_normalization_schemes_in_deep_networks%23:~:text=present%20a%20novel%20view%20on,precision%20implementations) [[19\]](https://www.researchgate.net/publication/323571095_Norm_matters_efficient_and_accurate_normalization_schemes_in_deep_networks%23:~:text=regularization%20effect%20studied%20in%20classical,training%20dynamics%20in%20a%20desirable) [[31\]](https://www.researchgate.net/publication/323571095_Norm_matters_efficient_and_accurate_normalization_schemes_in_deep_networks%23:~:text=,) Norm matters: efficient and accurate normalization schemes in deep networks | Request PDF

https://www.researchgate.net/publication/323571095_Norm_matters_efficient_and_accurate_normalization_schemes_in_deep_networks

[[7\]](https://arxiv.org/html/2506.02285v1%23:~:text=Loshchilov%20and%20Hutter,%20,1) [[11\]](https://arxiv.org/html/2506.02285v1%23:~:text=We%20utilize%20an%20existing%20theoretical,the%20weight%20norm%20during%20training) [[12\]](https://arxiv.org/html/2506.02285v1%23:~:text=During%20long,lower%20loss%20values%20throughout%20training) [[13\]](https://arxiv.org/html/2506.02285v1%23:~:text=We%20propose%20a%20theory,improve%20over%20the%20base%20optimizers) [[15\]](https://arxiv.org/html/2506.02285v1%23:~:text=show%20that%20weight%20decay%20controls,the%20weight%20norm%20during%20training) [[18\]](https://arxiv.org/html/2506.02285v1%23:~:text=Kosson%20et%C2%A0al,blowup%20when%20training%20in%20bfloat16) Why Gradients Rapidly Increase Near the End of Training

https://arxiv.org/html/2506.02285v1

[[8\]](https://proceedings.neurips.cc/paper/2020/file/a7453a5f026fb6831d68bdc9cb0edcae-Supplemental.pdf%23:~:text=distribution%20in%20the%20function%20space,main%20contributions%20are%20the%20following) proceedings.neurips.cc

https://proceedings.neurips.cc/paper/2020/file/a7453a5f026fb6831d68bdc9cb0edcae-Supplemental.pdf

[[9\]](https://arxiv.org/html/2510.15262v1%23:~:text=Recent%20works%20(Kosson%20et%C2%A0al,update%20behaves%20like%20a%20rotation) [[10\]](https://arxiv.org/html/2510.15262v1%23:~:text=The%20results,%20shown%20in%20Figure,shape%20remains%20stable%20across%20runs) [[20\]](https://arxiv.org/html/2510.15262v1%23:~:text=Maximal,to%20the%20model%20width) [[21\]](https://arxiv.org/html/2510.15262v1%23:~:text=magnitudes%20are%20dominated%20by%20the,Their%20prescription%20yields) [[32\]](https://arxiv.org/html/2510.15262v1%23:~:text=Image:%20Refer%20to%20caption) [[33\]](https://arxiv.org/html/2510.15262v1%23:~:text=Although%20the%20alignment%20factor%20cannot,rule%20for%20the%20weight%20decay) [[34\]](https://arxiv.org/html/2510.15262v1%23:~:text=Although%20the%20alignment%20factor%20cannot,rule%20for%20the%20weight%20decay) [[35\]](https://arxiv.org/html/2510.15262v1%23:~:text=matrix,shows%20the%20corresponding%20singular%20value) Robust Layerwise Scaling Rules by Proper Weight Decay Tuning

https://arxiv.org/html/2510.15262v1

[[14\]](https://kexue.fm/archives/11307%23:~:text=%E5%9C%A8%E3%80%8A%E4%B8%BA%E4%BB%80%E4%B9%88Adam%E7%9A%84Update%20RMS%E6%98%AF0,Across%20Neural%20Networks%E3%80%8B%E4%B8%AD%E3%80%82%E9%98%85%E8%AF%BB%E5%90%8E%EF%BC%8C%E7%AC%94%E8%80%85%E5%8F%91%E7%8E%B0%E5%85%B6%E4%B8%AD%E4%B8%8D%E4%BB%85%E5%8C%85%E5%90%AB%E4%BA%86Update%20RMS%E7%9A%84%E4%BC%B0%E8%AE%A1%EF%BC%8C%E8%BF%98%E5%8C%85%E5%90%AB%E4%BA%86Weight%20RMS%E7%9A%84%E4%BC%B0%E8%AE%A1%E3%80%82) AdamW的Weight RMS的渐近估计（上） - 科学空间|Scientific Spaces

https://kexue.fm/archives/11307

[[16\]](https://openreview.net/pdf?id=m51BgoqvbP%23:~:text=phenomenon,%20we%20conjecture%20that%20pretraining,Under%20this%20assumption) [[17\]](https://openreview.net/pdf?id=m51BgoqvbP%23:~:text=the%20sustained%20high%20learning%20rate,only%20one%20main%20branch,%20where) openreview.net

https://openreview.net/pdf?id=m51BgoqvbP

[[22\]](https://arxiv.org/html/2510.19093v1%23:~:text=alignment%20of%20a%20layer%E2%80%99s%20inputs,practice%20such%20as%20why%20P) [[23\]](https://arxiv.org/html/2510.19093v1%23:~:text=Image:%20Refer%20to%20caption%20,between,%20see%20Appx.%C2%A0B) [[24\]](https://arxiv.org/html/2510.19093v1%23:~:text=Despite%20this%20success,%20several%20works%C2%A0,scaled%20in%20the%20formulation%20above) [[25\]](https://arxiv.org/html/2510.19093v1%23:~:text=In%20Appendix%C2%A0B%20we%20explore%20the,reason%20learning%20rates%20somewhat%20transfer) [[26\]](https://arxiv.org/html/2510.19093v1%23:~:text=In%20Appendix%C2%A0B%20we%20explore%20the,reason%20learning%20rates%20somewhat%20transfer) Weight Decay may matter more than P for Learning Rate Transfer in Practice

https://arxiv.org/html/2510.19093v1

[[27\]](http://papers.neurips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf%23:~:text=,those%20weight%20vectors%20from%20their) [PDF] A Simple Reparameterization to Accelerate Training of Deep Neural ...

http://papers.neurips.cc/paper/6114-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.pdf

[[30\]](https://dl.acm.org/doi/pdf/10.5555/3326943.3327143%23:~:text=,to%20a%20more%20stable) Norm matters: efficient and accurate normalization schemes in deep ...

https://dl.acm.org/doi/pdf/10.5555/3326943.3327143